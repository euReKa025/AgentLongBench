<div align="center">

# AgentLongBench: A Controllable Benchmark for Long-Context Agents

### ğŸ“„ [**Read Paper**](https://arxiv.org/abs/2601.20730) ï½œ ğŸ¤— [**HuggingFace Dataset**](https://huggingface.co/datasets/ignis/AgentLongBench) ï½œ ğŸŒ [**Project Page**](https://github.com/euReKa025/AgentLongBench)

</div>

<br>

> **AgentLongBench** is the first benchmark designed to evaluate **Long-Context Agents** through simulated environment rollouts. Unlike traditional retrieval benchmarks, it assesses an agent's ability to perform **dynamic information synthesis**, **state tracking**, and **non-linear reasoning** across contexts ranging from **32K to 4M tokens**.

---

## ğŸš€ Key Features

- **Dynamic Interaction**: Evaluates agents via "Environment Rollouts" based on Lateral Thinking Puzzles, moving beyond static document QA.
- **Extreme Context Length**: Supports scalable context evaluation from **32K** up to **4M tokens**.
- **Controllable Difficulty**:
    - **Knowledge-Intensive (KI)**: Uses real-world entities (PokÃ©mon) to test parametric memory.
    - **Knowledge-Free (KF)**: Uses symbolic masking to strictly test in-context reasoning.
- **Information Density Tests**:
    - **Concise-Response**: Hundreds of interaction rounds, testing memory fragmentation.
    - **Verbose-Response**: High-density tool logs, testing needle-in-noise retrieval.

---

## ğŸ† Main Results

Below is the performance heatmap for the **Knowledge-Intensive & Concise-Response** setting. For full results, please refer to our [Paper](./agentlongbench.pdf).

<p align="center">
  <img src="en_intersection.png" width="90%" alt="Main Heatmap" />
</p>

---

## ğŸ“‚ Dataset & Labels

### 1. Standard Labels
We use standardized abbreviations in file paths and logs to distinguish settings.

| Dimension | Label | Full Name | Description |
| :--- | :--- | :--- | :--- |
| **Knowledge** | `ki` | Knowledge Intensive | Relies on external knowledge (e.g., PokÃ©mon names). |
| | `kf` | Knowledge Free | Abstract entities (e.g., `Item_84`) to isolate reasoning. |
| **History** | `c` | Concise-Response | Filtered tool outputs; creates long interaction chains. |
| | `v` | Verbose-Response | Raw/Noisy tool outputs; tests info filtering. |

### 2. Task Taxonomy
[cite_start]Tasks are categorized by the information source required to answer[cite: 176].

| Category | Tasks | Description |
| :--- | :--- | :--- |
| **ğŸ› ï¸ Tool Response** | `Count Frequency`, `Find Duplicates`, `Find Target Offsets` | Requires parsing precise details from machine-generated logs. |
| **ğŸŒ Env Response** | `Count Correctness`, `Count Frequency`, `Find Round with Largest Value`, `Weighted Summation` | Requires tracking state changes and feedback constraints. |
| **ğŸ§  Final Guess** | `Intersection` | The ultimate test of global understanding and logical deduction. |

### 3. Data Layout
The dataset structure is organized by **Setting** $\rightarrow$ **Length** $\rightarrow$ **Category**.

```text
benchmark/
  â”œâ”€â”€ ki-c/                  # Knowledge Intensive + Concise
  â”œâ”€â”€ ki-v/                  # Knowledge Intensive + Verbose
  â”œâ”€â”€ kf-c/                  # Knowledge Free + Concise
  â””â”€â”€ kf-v/                  # Knowledge Free + Verbose
       â””â”€â”€ <length>/         # e.g., 128k, 1M, 4M
            â”œâ”€â”€ tool_response/
            â”‚    â””â”€â”€ <question_type>.jsonl
            â”œâ”€â”€ env_response/
            â”‚    â””â”€â”€ <question_type>.jsonl
            â””â”€â”€ final_guess/
                 â””â”€â”€ intersection.jsonl
```




### ğŸ“¥ Download

The dataset is not included in the repo due to size. Please download it from Hugging Face and place it under `agentlong_bench/benchmark/`:

```bash
# Example structure after download
agentlong_bench/benchmark/ki-c/...

```

---

## âš¡ Quick Start

### Prerequisites

* Python 3.8+
* vLLM (for offline inference)

### Running Evaluation

From the `AgentLongBench` repository root, run a single-file eval (online
API runner) using the provided helper script:

```bash
bash scripts/eval_one.sh
```

Run a single-file offline vLLM evaluation:

```bash
bash scripts/run_vllm_one.sh
```

## ğŸ“ Citation
If you find this work useful, please cite our paper:

```bibtex
@misc{fang2026agentlongbenchcontrollablelongbenchmark,
      title={AgentLongBench: A Controllable Long Benchmark For Long-Contexts Agents via Environment Rollouts}, 
      author={Shicheng Fang and Yuxin Wang and XiaoRan Liu and Jiahao Lu and Chuanyuan Tan and Xinchi Chen and Yining Zheng. Xuanjing Huang and Xipeng Qiu},
      year={2026},
      eprint={2601.20730},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2601.20730}, 
}
```



